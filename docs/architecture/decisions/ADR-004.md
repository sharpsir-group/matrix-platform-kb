# ADR-004: Why Databricks for ETL over dbt/Airbyte/Fivetran

## Status

Accepted

## Context

The platform requires an ETL pipeline to ingest data from 3 sources across 3 markets. Options considered:

- **dbt + Snowflake**: SQL-first, strong for transformations; Snowflake adds cost
- **Airbyte + PostgreSQL**: Open-source connectors, but limited transformation layer
- **Fivetran**: Managed connectors, higher cost, less control
- **Databricks**: Unified analytics platform with Spark, SQL, and ML

Requirements included: schema normalization to RESO DD, CDC (Change Data Capture) support, and future ML use cases (recommendations, pricing models).

## Decision

Use **Databricks** with a medallion architecture (Bronze/Silver/Gold) for the ETL pipeline. Databricks provides:

- Delta Lake for ACID transactions and time travel
- SQL and PySpark for transformations
- Notebooks for CDC logic and custom connectors
- Free Community Edition for development and small-scale runs

## Consequences

**Positive**

- Powerful SQL analytics and ML training in one platform
- Medallion layers enable clear data lineage and incremental processing
- Free Community Edition reduces cost for current scale
- Delta Lake supports schema evolution and CDC patterns

**Negative**

- Higher complexity than dbt-only workflows
- CDC implemented via custom notebooks, not built-in connectors like Fivetran
- Databricks runtime and API changes may require pipeline updates
